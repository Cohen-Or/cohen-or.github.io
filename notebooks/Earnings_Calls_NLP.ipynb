{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRwlfxahqQvr"
      },
      "source": [
        "**Interpreting Earnings Calls - Advancing Portfolio Management with LLMs**\n",
        "\n",
        "**Author: Or Cohen**\n",
        "\n",
        "In this notebook we will implement a methodology developed by S&P Global Research for deriving a couple of qualtative alpha factors from the transcripts of earnings calls. It's highly recommended to switch to a GPU runtime before running this notebook. For a full explanation of the methodology please read the [blog post](https://cohen-or.github.io/posts/ec_nlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bjJFtiNjczqK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVMbrIDEuK1b",
        "outputId": "e9fc48cd-2b63-4aec-e764-0c88de556350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain\n",
        "!pip install -U -q langchain-community\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_md\n",
        "!pip install -q transformers\n",
        "!pip install -q accelerate\n",
        "!pip install -U -q bitsandbytes\n",
        "\n",
        "!pip install -q opendatasets\n",
        "!pip install -q fpdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0lAI08k_N_b",
        "outputId": "e040ee28-6d45-4edf-87a4-d22b81b8abd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping, found downloaded files in \"./motley-fool-scraped-earnings-call-transcripts\" (use force=True to force download)\n"
          ]
        }
      ],
      "source": [
        "# IF YOU DON'T HAVE A KAGGLE ACCOUNT - TRY ENTERING \"O\" FOR BOTH username and password\n",
        "import opendatasets as od\n",
        "dataset_url = \"https://www.kaggle.com/datasets/tpotterer/motley-fool-scraped-earnings-call-transcripts/data\"\n",
        "od.download(dataset_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xPLweiA5dPPQ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_pickle('/content/motley-fool-scraped-earnings-call-transcripts/motley-fool-data.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XNAhFzZLlrU"
      },
      "source": [
        "### Part 1 - spliting the transcript to dictionary of contextual blcoks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vcm2nU6SVDCc"
      },
      "outputs": [],
      "source": [
        "slctd_ec = df.loc[18491,'transcript']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RGeeifc2VWI4"
      },
      "outputs": [],
      "source": [
        "count_words = lambda line: len(line.split())\n",
        "\n",
        "def parse_qa_section(text):\n",
        "    # enquirer_pattern =\n",
        "    lines = text.strip().split('\\n')\n",
        "    qa_pairs = {}\n",
        "    current_qa = None\n",
        "    parsing_state = 'looking_for_question'\n",
        "    pair_counter = 1\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        # Check for question speaker line with word count validation\n",
        "        if parsing_state == 'looking_for_question' and (line.count('--') == 2 and count_words(line) <= 10):\n",
        "            current_qa = {\n",
        "                'enquirer': line,\n",
        "                'question_lines': [],\n",
        "                'responder': None,\n",
        "                'answer_lines': []\n",
        "            }\n",
        "            parsing_state = 'collecting_question'\n",
        "            continue\n",
        "\n",
        "        # Collect question lines\n",
        "        if parsing_state == 'collecting_question':\n",
        "            # Check if this is a new speaker line (responder)\n",
        "            if  line.count('--') >= 1 and count_words(line.split('--')[0]) == 2:\n",
        "                  current_qa['responder'] = line\n",
        "                  parsing_state = 'collecting_answer'\n",
        "                  continue\n",
        "\n",
        "            # Collect question lines\n",
        "            current_qa['question_lines'].append(line)\n",
        "            continue\n",
        "\n",
        "        # Collect answer lines\n",
        "        if parsing_state == 'collecting_answer':\n",
        "            # Check if this is the end of the answer\n",
        "            if line == \"Operator\\n\" or ((\"--\" in line) and (\"Investor Relations\" in line)) or (line.count('--') == 2 and count_words(line) <= 10):\n",
        "                # Save current Q&A pair\n",
        "                current_qa['question'] = '\\n'.join(current_qa['question_lines']).strip()\n",
        "                current_qa['answer'] = '\\n'.join(current_qa['answer_lines']).strip()\n",
        "\n",
        "                # Remove intermediate line lists\n",
        "                del current_qa['question_lines']\n",
        "                del current_qa['answer_lines']\n",
        "\n",
        "\n",
        "                qa_pairs[pair_counter] = current_qa\n",
        "                pair_counter += 1\n",
        "\n",
        "\n",
        "                if line == \"Operator\\n\" or ((\"--\" in line) and (\"Investor Relations\" in line)):\n",
        "                  parsing_state = 'looking_for_question'\n",
        "                else:\n",
        "                  parsing_state = 'collecting_question'\n",
        "                  # Reset for new Q&A pair\n",
        "                  current_qa = {\n",
        "                      'enquirer': line,\n",
        "                      'question_lines': [],\n",
        "                      'responder': None,\n",
        "                      'answer_lines': []\n",
        "                  }\n",
        "                continue\n",
        "\n",
        "            # Collect answer lines\n",
        "            current_qa['answer_lines'].append(line)\n",
        "\n",
        "    # Handle the last Q&A pair\n",
        "    if current_qa and current_qa['question_lines']:\n",
        "        current_qa['question'] = '\\n'.join(current_qa['question_lines']).strip()\n",
        "        current_qa['answer'] = '\\n'.join(current_qa['answer_lines']).strip()\n",
        "\n",
        "        # Remove intermediate line lists\n",
        "        del current_qa['question_lines']\n",
        "        del current_qa['answer_lines']\n",
        "\n",
        "        qa_pairs[pair_counter] = current_qa\n",
        "\n",
        "    return qa_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tHp-WXzi-RuS"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Remove RTF formatting if needed (basic cleanup)\n",
        "slctd_ec = re.sub(r'\\\\[a-zA-Z0-9]+', '', slctd_ec)  # Remove RTF commands\n",
        "slctd_ec = re.sub(r'\\{\\}', '', slctd_ec)  # Remove empty braces\n",
        "slctd_ec = re.sub(r'\\n+', '\\n', slctd_ec)  # Normalize newlines\n",
        "\n",
        "# Define improved regex patterns\n",
        "prepared_remarks_pattern = r\"Prepared Remarks:(.*?)Questions (&|and) Answers:\"\n",
        "qa_section_pattern = r\"Questions (&|and) Answers:(.*)Call participants:\"\n",
        "\n",
        "# Extract prepared remarks\n",
        "prepared_remarks_match = re.search(prepared_remarks_pattern, slctd_ec, re.DOTALL)\n",
        "prepared_remarks = prepared_remarks_match.group(1).strip() if prepared_remarks_match else \"\"\n",
        "\n",
        "# Extract Q&A section\n",
        "qa_section_match = re.search(qa_section_pattern, slctd_ec, re.DOTALL)\n",
        "qa_section = qa_section_match.group(2).strip() if qa_section_match else \"\"\n",
        "\n",
        "# Parse Q&A section\n",
        "qa_dict = parse_qa_section(qa_section)\n",
        "\n",
        "# Store results in a dictionary\n",
        "result = {\n",
        "    \"Prepared Remarks\": prepared_remarks,\n",
        "    \"Q&A\": qa_dict\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM1fCvK0MUVA"
      },
      "source": [
        "### Part 2 - \"Cleaning\" the transcript and summarizing the questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "E98Yqf-tXMIi"
      },
      "outputs": [],
      "source": [
        "# filter out short exchanges (expressions of gratitude and politeness)\n",
        "qa_dict_filtered = {}\n",
        "remove_keys = []\n",
        "for key, value in qa_dict.items():\n",
        "  question_sentences = list(nlp(value[\"question\"]).sents)\n",
        "  answer_sentences = list(nlp(value[\"answer\"]).sents)\n",
        "  if len(question_sentences) > 2 and len(answer_sentences) > 2 \\\n",
        "  and '\\nOperator\\n' not in value[\"answer\"] and \"Investor Relations\" not in value['responder']:\n",
        "    qa_dict_filtered[key] = value\n",
        "qa_dict = qa_dict_filtered\n",
        "result['Q&A'] = qa_dict_filtered\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkwSu_MGAYCw",
        "outputId": "87871f03-7699-47e5-c698-bf9e7719fe8e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "def summarize_question(question):\n",
        "    summary = summarizer(question, max_length=60, do_sample=False)\n",
        "    return summary[0]['summary_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "De9PmU3fMmPg",
        "outputId": "d86fb7e5-7f78-46c9-f8d7-5aa2465de6cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        }
      ],
      "source": [
        "for key, value in qa_dict.items():\n",
        "  qa_dict[key]['question_summary'] = summarize_question(value['question'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7HPKbS7_VgK"
      },
      "source": [
        "### Part 3 - Generate RAG answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQuBFTr57JOC"
      },
      "source": [
        "**Converting 'prepared remarks' section to embddeings and storing in a vector database (FAISS)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TQQaEV4S7aUC"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ybDEyQbY5uuN"
      },
      "outputs": [],
      "source": [
        "# Process the text with spacy to get sentences\n",
        "doc = nlp(result['Prepared Remarks'])\n",
        "remarks_sentences = [sent.text for sent in doc.sents]\n",
        "\n",
        "# Load the pre-trained sentence transformer model\n",
        "embedding_model = \"all-MiniLM-L6-v2\"\n",
        "model = SentenceTransformer(embedding_model)\n",
        "\n",
        "# Encode the sentences into embeddings\n",
        "embeddings = model.encode(remarks_sentences)\n",
        "embeddings = np.array(embeddings).astype('float32')\n",
        "\n",
        "# Prepare FAISS index\n",
        "# We will use the L2 distance index (faiss.IndexFlatL2) since we're working with dense vectors.\n",
        "emb_dimension = embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatL2(emb_dimension)\n",
        "faiss_index.add(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pXVUAokYteSx"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import login\n",
        "\n",
        "# # Set your token programmatically\n",
        "# login(token=\"YOUR_TOKEN_HERE\")\n",
        "\n",
        "# def load_llm_model():\n",
        "#     # Configure quantization\n",
        "#     quantization_config = BitsAndBytesConfig(\n",
        "#         load_in_4bit=True,\n",
        "#         bnb_4bit_compute_dtype=torch.float16,\n",
        "#         llm_int8_enable_fp32_cpu_offload=True\n",
        "#     )\n",
        "\n",
        "#     # Load tokenizer and model for Llama 3.1 8B\n",
        "#     model_id = \"meta-llama/Llama-3.1-8B\"  # Update model ID for Llama 3.1 8B\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "#     model = AutoModelForCausalLM.from_pretrained(\n",
        "#         model_id,\n",
        "#         device_map=\"auto\",\n",
        "#         torch_dtype=torch.float16,\n",
        "#         quantization_config=quantization_config\n",
        "#     )\n",
        "\n",
        "#     return tokenizer, model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "soQ4OAowtMmY"
      },
      "outputs": [],
      "source": [
        "def load_llm_model():\n",
        "    # Configure quantization\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    model_id = \"microsoft/phi-2\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Om-mUz1K_arD"
      },
      "outputs": [],
      "source": [
        "# Initialize RAG system\n",
        "class RAGSystem:\n",
        "    def __init__(self, faiss_index, remarks_sentences, embedding_model_name, tokenizer, llm_model):\n",
        "        self.faiss_index = faiss_index\n",
        "        self.context = remarks_sentences\n",
        "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = llm_model\n",
        "\n",
        "    def retrieve_context(self, query, previous_answer, top_k=10):\n",
        "            if previous_answer:\n",
        "              previous_answer_sents = [span.text for span in previous_answer]\n",
        "              # Add the previous answers to the context (prepared remarks and all question before)\n",
        "              self.context += previous_answer_sents\n",
        "\n",
        "              # Embed the previous answer and add to FAISS index\n",
        "              previous_answer_embeddings = self.embedding_model.encode(previous_answer_sents)\n",
        "              previous_answer_embeddings = np.array(previous_answer_embeddings).astype('float32')\n",
        "              faiss_index.add(previous_answer_embeddings)\n",
        "\n",
        "            # Embed the query\n",
        "            query_embedding = self.embedding_model.encode([query])[0]\n",
        "            query_embedding = np.array([query_embedding]).astype('float32')\n",
        "\n",
        "            # Perform the FAISS search\n",
        "            D, I = faiss_index.search(query_embedding, top_k)\n",
        "\n",
        "            # Retrieve the corresponding sentences (context) from FAISS\n",
        "            retrieved_texts = [self.context[idx] for idx in I[0]]\n",
        "\n",
        "            return retrieved_texts\n",
        "\n",
        "    def generate_answer(self, query, context):\n",
        "        # Format prompt with context and query\n",
        "        prompt = f\"\"\"\n",
        "        From the perspective of a top executive, answer the following question raised by a financial analyst during an earnings conference call based on the\n",
        "        following information.\n",
        "        Information:\n",
        "        {' '.join(context)}\n",
        "        Question: {query}\n",
        "        \"\"\"\n",
        "\n",
        "        # Tokenize the prompt\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=128,\n",
        "                temperature=0.01,\n",
        "                top_p=0.9,\n",
        "                do_sample=True\n",
        "            )\n",
        "\n",
        "        # Decode and return the response\n",
        "        response = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "        return response\n",
        "\n",
        "    def answer_question(self, query, previous_answers):\n",
        "        # Retrieve relevant context (prepared remarks + previous answers)\n",
        "        context = self.retrieve_context(query, previous_answers)\n",
        "\n",
        "        # Generate answer based on retrieved context\n",
        "        answer = self.generate_answer(query, context)\n",
        "\n",
        "        return answer, context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f1b924a2f7aa461ba6efd24cd637a755",
            "7dda8958c166487aa303bfb1b7c80c5f",
            "99811f13b44145299009c68925a10cd6",
            "469393c0f3bb41bd927bfc9b5c2b8b93",
            "f0590e2d0793471c91d2996d0281a23c",
            "72e67246ec964e19a7f2cdb3140914d2",
            "61213761bacf4f98a18325f62a66f86a",
            "14451a8ef1b44fcaa2637d5e6e5a51e8",
            "700b5593e3ce47d18490288915321e8b",
            "951027859f6f4d9287b8afac85ac060b",
            "4ba9642c730440498026b72f2092850e"
          ]
        },
        "id": "CXovRBBqDbhy",
        "outputId": "9d5aa0ab-f2fb-4f09-acb9-2733e1f03b0d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1b924a2f7aa461ba6efd24cd637a755",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create RAG system\n",
        "tokenizer, model = load_llm_model()\n",
        "rag = RAGSystem(faiss_index, remarks_sentences, embedding_model, tokenizer, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuCuPrY1oSZL",
        "outputId": "a91217ff-829c-4d1b-b0f4-d5be18d421d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "previous_answer = []  # Initialize an empty list to keep the sentences from the previous answer\n",
        "\n",
        "for q_id in qa_dict.keys():\n",
        "    query = qa_dict[q_id]['question']\n",
        "    answer, context = rag.answer_question(query, previous_answer)\n",
        "    qa_dict[q_id]['rag_answer'] = answer\n",
        "    qa_dict[q_id]['context'] = context\n",
        "    # Store the sentences in the current answer to be passed in the next iteration\n",
        "    previous_answer = list(nlp(qa_dict[q_id]['answer']).sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Q9MXGiZgVy"
      },
      "source": [
        "### Part 5 - Calculate Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hOy_8dG5rM3S"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def avg_cosine_similarity(question, answer):\n",
        "    \"\"\"\n",
        "    Compute the average cosine similarity between each pair of sentences\n",
        "    from the question and answer inputs using spaCy.\n",
        "\n",
        "    Parameters:\n",
        "    - question (str): First text block.\n",
        "    - answer (str): Second text block.\n",
        "\n",
        "    Returns:\n",
        "    - float: The average cosine similarity between all sentence pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    nlp = spacy.load(\"en_core_web_md\")  # Use medium-sized model for better accuracy\n",
        "    # Process both text blocks using spaCy\n",
        "    doc1 = nlp(question)\n",
        "    doc2 = nlp(answer)\n",
        "\n",
        "    # Split both text blocks into sentences\n",
        "    sentences1 = [sent.text.strip() for sent in doc1.sents]\n",
        "    sentences2 = [sent.text.strip() for sent in doc2.sents]\n",
        "\n",
        "    # Generate sentence embeddings for each sentence\n",
        "    embeddings1 = np.array([sent.vector for sent in doc1.sents])\n",
        "    embeddings2 = np.array([sent.vector for sent in doc2.sents])\n",
        "\n",
        "    # Compute cosine similarity between each pair of sentence embeddings\n",
        "    similarities = cosine_similarity(embeddings1, embeddings2)\n",
        "\n",
        "    # Calculate the average similarity\n",
        "    avg_similarity = np.mean(similarities)\n",
        "\n",
        "    return avg_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8ERjec1DUmMy"
      },
      "outputs": [],
      "source": [
        "for q_id in qa_dict.keys():\n",
        "  qa_dict[q_id]['on_topic_score'] = avg_cosine_similarity(qa_dict[q_id]['question_summary'], qa_dict[q_id]['answer'])\n",
        "  qa_dict[q_id]['proactive_score'] = avg_cosine_similarity(qa_dict[q_id]['question_summary'], qa_dict[q_id]['rag_answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0rmXQ11lFcny"
      },
      "outputs": [],
      "source": [
        "# Calculate the call-wise average scores\n",
        "result['on_topic_score'] = np.mean([qa_dict[q_id]['on_topic_score'] for q_id in qa_dict.keys()])\n",
        "result['proactive_score'] = np.mean([qa_dict[q_id]['proactive_score'] for q_id in qa_dict.keys()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSQf7BOLk40l"
      },
      "source": [
        "### Export to PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Qw0RaGLKgKQk"
      },
      "outputs": [],
      "source": [
        "from fpdf import FPDF\n",
        "\n",
        "def export_to_pdf(result_dict, filename=\"report2.pdf\"):\n",
        "    \"\"\"Exports the content of a dictionary to a PDF report.\n",
        "\n",
        "    Args:\n",
        "        result_dict (dict): The dictionary containing the content to be exported.\n",
        "        filename (str, optional): The name of the PDF file. Defaults to \"report.pdf\".\n",
        "    \"\"\"\n",
        "\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "\n",
        "    for key, value in result_dict.items():\n",
        "        if isinstance(value, dict):  \n",
        "            if key == \"Q&A\":  # Special handling for Q&A section\n",
        "                for question_number, question_data in value.items():\n",
        "                    # Iterate through each part (question, answer, etc.) inside the question dictionary\n",
        "                    for part, part_value in question_data.items():\n",
        "                        combined_key = f\"{question_number}-{part}\"\n",
        "                        pdf.set_text_color(255,0,0)\n",
        "                        pdf.cell(200, 10, txt=combined_key + \":\", ln=True)\n",
        "                        pdf.ln()  \n",
        "                        pdf.set_text_color(0,0,0)\n",
        "                        pdf.multi_cell(0, 10, txt=str(part_value))\n",
        "                        pdf.ln()  \n",
        "\n",
        "        else:\n",
        "            # Handle other regular key-value pairs\n",
        "            pdf.set_text_color(0,255,0)\n",
        "            pdf.cell(200, 10, txt=str(key) + \":\", ln=True)\n",
        "            pdf.ln()  # Blank line after the key\n",
        "\n",
        "            pdf.set_text_color(0,0,0)\n",
        "            pdf.multi_cell(0, 10, txt=str(value))\n",
        "            pdf.ln()\n",
        "\n",
        "    # Output the PDF\n",
        "    pdf.output(filename)\n",
        "\n",
        "\n",
        "export_to_pdf(result)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14451a8ef1b44fcaa2637d5e6e5a51e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "469393c0f3bb41bd927bfc9b5c2b8b93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_951027859f6f4d9287b8afac85ac060b",
            "placeholder": "​",
            "style": "IPY_MODEL_4ba9642c730440498026b72f2092850e",
            "value": " 4/4 [01:39&lt;00:00, 21.11s/it]"
          }
        },
        "4ba9642c730440498026b72f2092850e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61213761bacf4f98a18325f62a66f86a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "700b5593e3ce47d18490288915321e8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72e67246ec964e19a7f2cdb3140914d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dda8958c166487aa303bfb1b7c80c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72e67246ec964e19a7f2cdb3140914d2",
            "placeholder": "​",
            "style": "IPY_MODEL_61213761bacf4f98a18325f62a66f86a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "951027859f6f4d9287b8afac85ac060b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99811f13b44145299009c68925a10cd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14451a8ef1b44fcaa2637d5e6e5a51e8",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_700b5593e3ce47d18490288915321e8b",
            "value": 4
          }
        },
        "f0590e2d0793471c91d2996d0281a23c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1b924a2f7aa461ba6efd24cd637a755": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7dda8958c166487aa303bfb1b7c80c5f",
              "IPY_MODEL_99811f13b44145299009c68925a10cd6",
              "IPY_MODEL_469393c0f3bb41bd927bfc9b5c2b8b93"
            ],
            "layout": "IPY_MODEL_f0590e2d0793471c91d2996d0281a23c"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
