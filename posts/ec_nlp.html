<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>פענוח שיחות משקיעים - מודלי שפה לשכלול תיקי השקעות</title>
  <link href="/css/syntax.css" rel="stylesheet">
  <!-- MathJax: For LaTeX Rendering -->
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <style>
    ltr-list {
    direction: ltr;
    text-align: left;
    display: block;
    margin-left: auto;
    margin-right: auto;
    }

    body {
      margin: 0;
      padding: 0;
      height: 100%;
      background-color: white;
      padding: 20px;
      max-width: 800px;
      margin: 0 auto;
      border-radius: 8px;
      font-family: Arial, sans-serif;
    }

    body {
      color: #333;
      padding: 20px;
    }

    header {
      text-align: center;
      margin-bottom: 20px;
    }

    header h1 {
      margin: 0;
      font-size: 2.5rem;
    }

    footer {
      text-align: center;
      font-size: 0.9rem;
      color: #888;
      margin-top: 30px;
    }

    /* Syntax Highlighting for Rouge (code blocks) */
    pre code {
      display: block;
      background-color: #f5f5f5;
      padding: 10px;
      border-radius: 5px;
      overflow-x: auto;
      white-space: pre-wrap;
    }

    /* Custom styling for Python syntax highlighting */
    .language-python .keyword {
      color: #d14;  /* Color for Python keywords like def, return, etc. */
    }

    .language-python .function {
      color: #795da3;  /* Color for function names */
    }

    .language-python .string {
      color: #df0000;  /* Color for strings */
    }

    .language-python .number {
      color: #008000;  /* Color for numbers */
    }

    .language-python .comment {
      color: #888;  /* Color for comments */
      font-style: italic;
    }

    .language-python .variable {
      color: #0000ff;  /* Color for variables */
    }

    .language-python .operator {
      color: #000000;  /* Color for operators (e.g., `=`, `+`, `-`, etc.) */
    }

    /* Optional: Adjust background for Python code blocks specifically */
    .language-python {
      background-color: #f0f0f0;
    }
    /* Centering and resizing images */
    img {
      display: block;
      max-width: 100%;  /* Ensures images do not overflow container */
      height: auto;     /* Maintains aspect ratio */
      margin-left: auto;
      margin-right: auto;
    }

    body {
      direction: rtl;
      text-align: right;
    }
    p, h1, h2, h3, h4, h5, h6, li {
      direction: rtl;
      text-align: right;
    }
    /* p:lang(en), li:lang(en) {
      direction: ltr;
      text-align: left;
    } */
  </style>
</head>
<body>


  <p><a href="http://cohen-or.github.io">אור כהן</a></p>
  <h1 id="פענוח-שיחות-משקיעים---מודלי-שפה-לשכלול-תיקי-השקעות">פענוח שיחות משקיעים - מודלי שפה לשכלול תיקי השקעות</h1>
  <a target="_blank" href="https://colab.research.google.com/github/Cohen-Or/cohen-or.github.io/blob/607fab5a1540b1c952afe4c75bceeb4406c66645/notebooks/Earnings_Calls_NLP.ipynb">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
  </a>
  <p>בפוסט הזה נסקור מימוש של מודלי שפה גדולים (LLMs) לניתוח שיחות משקיעים (Earnings Calls) שפותח בחברת S&amp;P Global Research ובהמשך נעבור מהלכה למעשה ונצלול לטכניקה שבליבת התהליך.</p>
  <p>הסופר והפילוסוף הצרפתי וולטר טען שהשפה חשובה גם כיוון שהיא עוזרת לנו להסתיר את מחשבותינו. בשונה מהמספרים והעובדות המוחלטות שאנליסטים ומשקיעים מנתחים לעיפה, לאופן התקשורת ומידת השקיפות של מנהלים בכירים בנוגע לאתגרים עסקיים וסוגיות בעייתיות ניתן לרוב משקל דל, אם בכלל, בתהליך הניתוח של חברות, לאו דווקא מפאת חוסר חשיבות.</p>
  <h2 id="מתמליל-שיחה-לציון-תומך-החלטה">מתמליל שיחה לציון תומך החלטה</h2>
  <p>חברות בורסאיות גדולות נוהגות לקיים שיחת משקיעים בססמוך למועד פרסום הדו״חות הכספיים על אף שאינן מחויבות לכך רגולטורית. שיחת המשקיעים היא אירוע תקשורתי שמקיימת חברה ציבורית עם משקיעים קיימים ופוטנציאליים, אנליסטים וגורמים פיננסיים נוספים. מטרת השיחה היא לספק מידע שקוף ומקיף על ביצועי החברה, אסטרטגיה עסקית, תחזיות ונושאים רלוונטיים נוספים. רוב השיחות מתנהלות כשיחת וידאו וספקי מידע פיננסי שונים מאפשרים גישה לארכיון של הקלטות ותמלולים.</p>
  <p>בחלק הראשון של השיחה המנכ״ל.ית וצוות הניהול הבכיר סוקרים נושאים מרכזיים בפעילות העסקית ובתוצאות הכספיות ובחלק השני עונים על שאלות מצד האנליסטים. הדואט הזה בין ההנהלה והאנליסטים טומן בחובו מידע רב ערך למשקיעים, הן בשאלות והן בתשובות. לפי תוצאות מחקר [1] של חברת S&amp;P, כאשר שאלות האנליסטים קשורות לנושאים שלא נכללו בדברי ההנלה בפתח השיחה ו/או, כאשר התשובה מתחמקמת מלעסוק באופן ישיר בנושא הנשאל, תוחלת התשואה החזויה יורדת. מהצד השני, תקשורת שקופה ומענה ענייני מצד ההנהלה מאפיינים ניהול תקין והגון שבתורם מביאים לתשואות עודפות.</p>
  <h4 id="דוגמת-enron">דוגמת Enron</h4>
  <p>במאמר מוצגות כמה דוגמאות שממחישות היטב את התזה של החוקרים, הבולטת שבהן היא שיחת המשקיעים של חברת האנרגיה אנרון שהתקיימה בסמוך לפרסום הדו״ח הכספי לרבעון השני של שנת 2001. בשיחה אנליסט מבנק גולדמן זאקס שאל את המנכ״ל את השאלה הבאה בקשר להפחתות בגין ירידת ערך שביצעה החברה: ״כמה אנו יכולים להיות בטוחים שאלו יהיו ההפחתות האחרונות?״. המנכ״ל השיב ״אם היינו חושבים שיש מקום להפחתות נוספות בשווי הנכסים, הן היו נכללות בדו״ח הנוכחי. אבל יש לנו לפחות שלושה אזורי אי-ודאות בעסק, כמו שאתם יודעים. כמובן, אזור אחד הוא קליפורניה… יש את הודו… ואחריה כמובן, לסיום תחום הפס-רחב.״</p>
  <p>המונח ״הפחתות בגין ירידת ערך״ (Impairment Write Downs) לא נכלל בשום צורה בדברי הפתיח של ההנלה אבל כן הוזכר בשש מהשאלות ששאלו האנליסטים. תשובת המנכ״ל וגם התשובות האחרות לא שידרו ביטחון בלשון המעטה ונראו כמו תמרון התחמקות לא מוצלח. כחודש וחצי לאחר השיחה החברה הגישה מסמכי צ׳פטר 11 (פשיטת רגל) וכמה מחברי צוות ההנהלה נתבעו ע״י רשות ני״ע האמריקאית (SEC) על הונאה חשבונאית.</p>
  <p>דוגמאות נוספות שהובאו במאמר מראות איך העקרון עובד גם הפוך, כלומר לחיוב, כאשר שקיפות וענייניות נותנים ביטחון למשקיעים, מורידים את הסיכון הגלום בהשקעה בחברה, ובאופן מצטבר מביאים לתשואות עודפות. כמה עודפות? בגרף מטה מתואר הערך הכספי המצטבר בתקופה שמינואר 2008 עד ספטמבר 2024 של ארבע תיקי השקעה (פורטפוליוס) שנבחרו מתוך מדד מניות הראסל 3000 על בסיס  <strong>הפקטורים</strong> הבאים:</p>
  <ul>
  <li><strong>תקשורת פרואקטיבית / ראקטיבית</strong> - נקבע לפי רמת ההתאמה בין תוכן השאלות של האנליסטים ותוכן דברי הפתיח של ההנהלה.</li>
  <li><strong>תשובות ענייניות / פתלתלות</strong> - נקבע לפי רמת ההתאמה בין הנושא הנשאל ותוכן התשובה.</li>
  </ul>
  <p>בסוף כל חודש קלנדרי במהלך תקופת המבחן ההיסטורי (בקטסט), כל המניות במדד חולקו לתיקי השקעה עפ״י שני הפקטורים הנ״ל כאשר תיק הלונג (חשיפה חיובית) מורכב מהסל הכולל את שני העשירונים עליונים, תיק השורט (חשיפה שלילית) מורכב מהסל הכולל את שני עשירונים התחתונים ויתר החברות (60%) בקבוצת האמצע (חשיפה נייטרלית). החיבור של שני הפקטורים הנ״ל בלבד יוצרה מרווח מרשים בין תיק הלונג והשורט של 506 נקודות בסיס במונחים שנתיים.</p>
  <p><img src="/images/ec_nlp1.png" alt="מבחן היסטורי (בקטסט) של תיקים שנבחרו על בסיס הפקטורים במחקר"><br>
  מבחן היסטורי (בקטסט) של תיקים שנבחרו על בסיס הפקטורים במחקר. מקור: [1]</p>
  <p>רק כדי לוודא שכולנו על אותו דף, כשמדברים על <strong>פקטורים בעולם של חיזוי תשואות</strong> של תיקי מניות מתייחסים למשתנים מסבירים - כלומר משתנים מדידים כגון שווי שוק, מכפיל הון ומומנטום - שהוכחו כקורלטיביים לתשואה הצפויה. זהו למעשה שדרוג של המודל הקלאסי של מרקוביץ׳ שגרס שהתשואה בהשקעה במניה תלויה באופן ישיר ומוחלט בסיכון הגלום בה שנמדד באמצעות סטיית תקן ומחושב ביחס לשוק כולו. היחס הזה בין סיכון המניה לסיכון השוק מבוטא כמקדם בשם בטא וכאשר הבטא שווה 1 הסיכון של המניה זהה לסיכון שבתיק השוק וכאשר היא גבוהה (נמוכה) מ-1 אז התשואה יותר (פחות) רגישה לתנודות השוק. מאז המחקר פורץ הדרך של פמה ופרנץ׳ [2] שהציג את הפקטורים הראשונים  בתחילת שנות ה-90 נוספו מאות פקטורים מעבר לסיכון ביחס לשוק כמו גם מחקרים שהציעו שיטות סטטיסטיות להשוואה, סיווג ודירוג של שפע הפקטורים [3].</p>
  <p>בחזרה לפקטורים שהוצגו במחקר, באופן מרשים לא פחות מיכולות החיזוי של הפקטורים הנ״ל כאשר הם עומדים בפני עצמם, יכולת החיזוי הגבוהה נשמרת גם בשילוב עם פקטורים אחרים מבוססי עיבוד שפה טבעית (NLP) כמו סנטימנט חיובי ובקיאות מספרית. התובנה הזאת מחזקת את ההנחה הרווחת לפיה מודלי שפה גדולים יכולים להניב מגוון רחב של פקטורים בלתי תלויים ומהימנים ססטיסטית מתוך מקורות מידע כמו תמלולים של שיחות משקיעים וחוות דעת כתובות של אנליסטים. כדאי גם לציין שלכל אחד משני הפקטורים יש תרומה כמעט שווה ליכולת החיזוי הכוללת.</p>
  <p>להבדיל ממרבית הפקטורים שהוצגו במחקר עד היום, פקטורים מבוססי NLP מוסיפים <strong>מידע איכותני</strong> שלא היה ניתן לקבל מתוך המספרים בדו״חות הכספיים או בסדרות הזמן של המחיר. התוספת החשובה הזו נשענת על שילוב של ניסיון ותובנות מעמיקות של אנליסטים יחד עם ידע מתקדם בתחום הלמידת מכונה ועיבוד שפה טבעית. המאמר הזה הוא דוגמה פנטסטית למקרה שימוש שכולל מספר שלבים שתוכננו בתבונה ובקפידה כדי להביא תוצאה מהימנה.</p>
  <h2 id="הלכה-למעשה">הלכה למעשה</h2>
  <p>אם יש לכם ניסיון בסיסי בתכנות תוכלו לפתוח סביבת עבודה בענן (קישור בראש העמוד) עם גוגל Colab ולהתנסות בקוד שכתבתי שמבצע את התהליך על <a href="https://www.kaggle.com/datasets/tpotterer/motley-fool-scraped-earnings-call-transcripts/data">דאטא סט</a> של 18,755 תמלולים של שיחות משקיעים של כ-2900 חברות שונות שנערכו בין הרבעון השלישי של 2017 לרבעון השלישי של 2024. התהליך הזה דורש המון משאבי עיבוד (GPU) ולכן יישמתי כך שירוץ על תמלול בודד ולא על כל התמלולים.בהמשך ישיר, <em>ממליץ בחום להשתמש באפשרות של Colab להחליף לסביבה עם GPU.</em> לגבי המודל, במבחן שביצעו החוקרים הם השתמשו ב llama 3.1 8b של מטא (זמין דרך hugging face בהרשמה); אם אין לכן חשבון בhugging face תוכלו להתנסות בקוד עם מודל phi2 של מיקרוסופט אבל התשובות יהיו פחות איכותיות.</p>
  <p>הערה טכנית אחרונה: התמלולים כולם נאספו מאתר Motley Fool ופורסמו בKaggle; אפתח פה סוגריים ואציין שמבדיקה שערכתי מצאתי אתרים שמספקים תמלולים שמכילים לא מעט טעויות תמלול שכמובן יפגעו באיכות התוצאה ולכן מגיע ח״ח ל-Motley Fool שמשתמשים במודל התמלול של Verbit הישראלית שמביאה תוצאת תמלול פנומנלית.</p>
  <h3 id="השלבים-במבט-על">השלבים במבט על</h3>
  <p>באופן אישי אני מעדיף להבין מלמעלה למטה (Top-down) אז נתחיל בסקירה של החלקים המרכזיים בתהליך ומשם נצלול להבנה של הנושאים המעניינים. את התהליך כולו אפשר לחלק לחמשת ראשי הפרקים הבאים כפי שחילקתי בקוד:</p>
  <ol>
  <li><strong>הפרדה  של חלקי השיחה ו״ניקיון״</strong> - נפריד ונשמור במבנה נתונים מתאים את דברי הפתיח וכל צמד של שאלה ותשובה כדי לאפר שליפה יעילה. במהלך השו״ת ישנם לרוב קטעים של חילופי מילות תודה ונימוס בין האנליסטים וההנלה שכמובן אינם רלוונטים למטתרנו ולכן נסנן אותם. בהתחלה ניסיתי להשתמש בביטוי רגולרי (רגקס) אבל מהר מאוד גיליתי שלאינטרקציה בשיחות אין מבנה מסודר וצריך ליישם פונקציה לטובת העניין - הסבר קצר ל Claude וכמה תיקונים והמשכנו. שימו לב שזה לא Production Ready - לפעמים יש אנליסטים ששואלים שתי שאלות שונות ביחד מה שפוגע בהמשך התהליך. זה מיעוט ולא רלוונטי להמחשה אז נתעלם מזה.</li>
  <li><strong>תימצות השאלות</strong> -  המאמר מציע לתמצת את השאלות על מנת לשפר את יחס הסינל לרעש (Signal to Noise). במימוש שיצרתי נשתמש במודל <a href="https://huggingface.co/facebook/bart-large-cnn">BART</a> של מטא שמותאם במיוחד לשימושים כמו תימצות ותירגום.</li>
  <li><strong>יצירת תשובה באמצעות מודל שפה (LLM)</strong> - נשתמש בכל דברי ההנלה באותה השיחה עד לשאלה, כלומר דברי הפתיח והתשובות לכל השאלות שקדמו לאותה שאלה, כדי לסנתז תשובה עניינית שתהיה הפרוקסי להערכת הפרואקטיביות. בשלב זה, כדי לספק את המידע החדש למודל נשתמש בגישת ה-<em>RAG</em> ונכוון את המודל ל<em>טמפרטורה</em> מינימלית.</li>
  <li><strong>חישוב מידת ההתאמה בין כל שאלה ושתי התשובות</strong> - מידת ההתאמה בין השאלה לתשובה המג׳ונרטת היא ציון הפרואקטיביות ומידת ההתאמה בין השאלה והתשובה המקורית היא ציון הענייניות. בשלב הזה משתמשים בתכונה מהותית של <em>ההטעמה הוקטורית</em> של המשפטים.</li>
  <li><strong>חישוב ממוצע של שני הציונים</strong> - ציון ממוצע לפרואקטיבית ולענייניות על בסיס כל השאלות בשיחה.</li>
  </ol>
  <h3 id="הטמעה-וקטורית---vector-embedding">הטמעה וקטורית - Vector Embedding</h3>
  <p>שאלה מאוד אינטואיטיבית בהקשר של מודלי שפה היא איך בכלל מחשב מבין תוכן של משפט או תמונה. בשורש היכולת הזו חבויה שיטת ההטמעה וקטורית שבה למעשה ממירים אובייקטים (מילים, משפטים, תמונות או אפילו משתמשים) לוקטורים במרחב רב מימדי (יכול להיות 100 או 300  ויותר מימדים), כלומר סדרת מספרים, בדרך כזו שהאובייקטים הדומים יותר יהיו קרובים יותר זה לזה במרחב הווקטורי. במילים פשוטות, הכוונה היא לייצוג של אובייקטים במרחב מספרי, שבו כל אובייקט מקבל ייצוג כמערך (או וקטור) של מספרים. את המערכים הללו אפשר להשוות, למיין, ולבצע עליהם חישובים נוספים.</p>
  <p>את ההמרה הזו עושים באמצעות רשתות נוירונים (Neural Netowrks) שלומדות על בסיס המון דוגמאות של מידע לדייק את המערך הוקטורי שמייצג כל אובייקט. הוקטורים האלה הם הבסיס של הרבה מאוד יכולות בינה מלאכתותית כמו ניתוח שפה, חיפוש מבוסס משמעות (ולא רק מילים), זיהוי דמיון בין אוביקטים, סיפוק המלצות ועוד.</p>
  <p>כדי לא להשאיר אף קורא מאחור, רשתות נוירוניות (ר״נ) הן למעשה הלחם והחמאה של התחום שנקרא למידה עמוקה (Deep Learning). ר״נ זה בעצם שם מפואר לאלגוריתם שמכיל שרשרת (או רשת) של פונקציות, ברובן לא לינאריות. להבדיל מהפונקציות שהכרנו בשיעורי חדו״א, אוסף כזה של פונקציות הוא בלתי פתיר בשיטות פורמליות ולכן הייחוד של אלגוריתם הזה טמונה ביכולת לבצע אופטימיזציה של הפרמטרים בכל פונקציה בשרשרת (המשקולות והקבועים) כדי להגיע לתוצאה אופטימלית בהתאם לפונקצית הפסד או עלות (Loss / Cost function). תהליך האופטימזציה נקרא אימון ובמהלכו מנסים באופן אקראי צמדים של קלט (תמונות, משפטים, סדרות זמן) ופלט (וקטור מוטמע, סיווג, סיגנל פיננסי וכו׳). לרוב בין שכבת הקלט, השכבה הראשונה בשרשרת הפונקציות, ושכבת הפלט יש מספר ״שכבות נסתרות״ (hidden layers) כלומר פונקציות ביניים ומכאן השם ״למידה עמוקה״.</p>
  <p>בחזרה למאמר של S&amp;P, במותודולוגיה שהוצגה כל משפט בדברי הפתיח, בשאלות המתומצתות, ובכל אחת מהתשובות עובד באמצעות הטמעה וקטורית ונשמר במאגר (דאטה בייס) ייעודי לוקטורים (בקוד שלי השתמשתי בFAISS של מטא). כדי לחשב את מידת <strong>ההתאמה הסמנטית</strong> בין צמד מסוים של שאלה ותשובה מבצעים חישוב <strong>Cosine Similarity</strong> בין כל משפט בשאלה וכל משפט בתשובה (מכפלה קרטזית של המשפטים) וע״מ לסכם לתוצאה אחת לכל צמד של שאלה ותשובה מחשבים ממוצעצ מהתוצאות של כל הקומבינציות.</p>
  <p>Cosine Similarity הוא למעשה מדד לדמיון בין שני וקטורים במרחב שמתבסס על חישוב קוסינוס הזווית ביניהם. בפועל, אם שני וקטורים במרחב מצביעים לאותו הכיוון סימן שדהמיון ביניהם גבוה ולכן התוצאה תהיה קרובה ל 1 ולהבדיל אם הוקטורים הם בכיוון מנוגד סימן שהייצוג שלהם מנוגד והתוצאה תהיה קרובה ל 1-.</p>
  <p><img src="https://kdb.ai/files/2024/01/similarity-768x348.png" alt="המחשה של Cosine Similarity במרחב דו מימדי"><br>
  המחשה של Cosine Similarity במרחב דו מימדי. מקור: KDB.</p>
  <h3 id="יצירה-מבוססת-אחזור---rag">יצירה מבוססת אחזור - RAG</h3>
  <p>כזכור, על מנת לאמוד את מידת הפרואקטיביות המאמר הציג פתרון יצירתי ואינטילגנטי והוא שימוש במודל שפה כדי לייצר תשובה שמבוססת על דברי הפתיח והמידע שסיפקה ההנהלה בתשובות עד לאותה השאלה. בצורה הזו מסתנזים תשובה ברורה ותמציתית מכמה פיסות מידע שמפוזרות בנקודות שונות בשיחה, וכך מאפשרים השוואה יעילה אל מול השאלה.</p>
  <p>כדי להבין את החשיבות של יצירה מבוססת אחזור בתחום הבינה מלאכותית יוצרת (GenAI) ניקח רגע צעד אחורה כדי להבין מה בעצם קורה מאחורי הקלעים. כדי ללמד את המחשב ליצור על בסיס נתונים חוקרי ה-AI בחרו להשתמש במודל הסתברותי. ההנחה היתה ש<em>אם המחשב ילמד את הסטטיסטיקה של המציאות, הוא יידע ליצור דברים שנראים כאילו הם מציאותיים גם אם הם לא</em>. אבל המציאות היא לא רק הסתברותית אלא גם מוחלטת (דטרמיניסטית) וכשמשתמשים במודל הסתברותי כדי לחקות את המציאות, תמיד תהיה לו הסתברות ל"הזיות" (hallucinations). כך נוצרים איורי AI של ילד עם שלוש רגליים או טקסטים שמשרבבים שמות, מונחים או תקדימים משפטיים מומצאים - כולם תוצאה של דמיונו הפורה של הצ׳אט.</p>
  <p>בעיית ההזיות היתה מוכרת לחוקרים בתחום מזה שנים רבות אך בשנתיים האחרונות רוכז המאמץ במציאת פתרונות הודות לזרקור החזק שהופנה לעבר הבעיה בזכות החשיפה הגבוהה של המודלים ויכולותיהם, לטוב ולרע, שהביא ChatGPT. אחת הגישות המובילות לפתרון של בעיית ההזיות נקרא <strong>grounding</strong> (מעניין באיזה מונח תבחר האקדמיה ללשון) והרעיון שלו הוא להשתמש במודל כדי לחפש (לאחזר) מידע מתוך מקורות מהימנים ואחר-כך לסנתז תשובה מקיפה מתוך התוצאות המובילות בעזרת יכולות העיבוד שפה. ככה למעשה מונעים מהמודל ״לזייף״ מציאות סבירה סטטיסטית ומשיגים את הטוב שבשני העולמות - גם מהיכולת <em>ליצור תוכן מותאם</em> וגם מהיכולת <em>לחפש מידע רלוונטי על בסיס משמעות</em> (ולא רק התאמה בין המילים).</p>
  <p><img src="https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/jumpstart/jumpstart-fm-rag.jpg" alt="תרשים של החלקים והתהליכים ביצירת תוכן בגישת RAG"><br>
  תרשים של החלקים והתהליכים ביצירת תוכן בגישת RAG. מקור: AWS Documentation.</p>
  <h3 id="טמפרטורה-של-מודלי-שפה">טמפרטורה של מודלי שפה</h3>
  <p>הדרך הכי פשוטה להבין איך שינוי בפרמטר הטמפרטורה משפיע על תוצאות מודל שפה היא פשוט לשאול את ֿChatGPT או כמעט כל מודל אחר את אותה שאלה מספר פעמים ולהוסיף ״תענה בטמפרטורה X" כאשר X הוא מספר בין 0 ל1. מה זה עושה? כאמור ולאחר האימון כשמתמשים במודל ליצירה <strong>המודל משלים רצף טקסטואלי ע״י בחירת המילה (נקרא טוקן - Token) בעלת ההסתברות הגבוהה ביותר להתאמה סמנטית ותוכנית לחלק הבא ברצף</strong>. טמפרטורה נמוכה תביא לכך שהתוצאה שנקבל תהיה מורכבת מהמילים בעלות ההסתברות המקסימלית וככה בעצם נלך על בטוח ונקבל תשובה ״קרה״. להבדיל, טמפרטורה גבוהה תיתן למודל חופש מסוים לבחור מילים פחות צפויות ולתת תשובות יותר מקוריות.</p>
  <p>במקרה שימוש כמו זה שבמאמר כמובן שנעדיף טמרטורה נמוכה. גם לשאילתה (הפרומפט) בה השתמשו החוקרים (במסגרת מטה) יש תרומה חשובה לכיוונון המודל ולאופי התוצאות ובהקשר הזה כדי לשים לב ל-<strong>Knowledge cutoff</strong> שהוא פיצ׳ר של מודלי שפה מסוימים שמאפשר לייצר תוצאה על בסיס המידע שהיה קיים עד לאותו תאריך, כלומר להתעלם ממידע שנכלל בשלב האימון של המודל ונוצר לאחר התאריך הקובע, ובכך למנוע את הטיית החוכמה שבדיעבד (<em>Hindsight bias</em>) שהיתה מוכרת לאנשים בעולם השקעות עוד הרבה לפני עידן הבינה מלאכותית.</p>
  <blockquote>
  <p>From the perspective of a top executive, please answer the following<br>
  question raised by a financial analyst during an earnings conference<br>
  call. Knowledge cutoff: {Date of the Earnings Call}.</p>
  </blockquote>
  <h2 id="סיכום">סיכום</h2>
  <p>הפריצה של ChatGPT גרמה לרבים וטובים לתלות תקוות גדולות ביכולת שלו, ישר מהמדף, לפצח את האניגמה שנקראת שוק ההון ולתת המלצות שיניבו תשואות גבוהות. כתבתי פה בעבר על <a href="https://cohen-or.github.io/posts/sjm.html#demystifying-the-black-box">הסיכון שבהישענות על קופסאות שחורות שכאלו</a>, שלא מאפשרות בחינה וביקורת מקצועית של מומחה אנושי. המאמר הזה מדגים באופן נהדר את הפוטנציאל שיש במודלים האלו וממחיש את השינוי הטקטוני שמתחולל בעולם ההשקעות בדמות הסימביוזה בין מדעני הנתונים לבין אנליסטים פיננסים.</p>
  <p>לעניות דעתי ההתקדמות בתחום ה-AI ומודלי שפה תביא בשנים הקרובות להתחזקות של שתי מגמות עיקריות: הראשונה היא <em>מעבר מקרנות מחקות שעוקבות אחרי מדד משוקלל שווי שוק (Cap Weighted) לקרנות בטא חכמה</em>. כתבתי בעבר על <a href="https://cohen-or.github.io/posts/best_ideas.html#the-value-of-active-management-aka-stock-picking">בעיית הריכוזיות שיוצרות הקרנות המחקות</a>  והקשר שלה לעולם תורת המשחקים. קרנות בטא חכמה עוקבות אחרי מדדים שמאגדים כמה פקטורים, כמו אלו שהוצגו במאמר, כדי לפזר את ההון על פני מדד מסוים (לדוגמה סקטוריאלי או גיאוגרפי) בצורה שמייצרת תשואה עודפת על המדד ה״טיפש״.</p>
  <p>מגמה שנייה שלהערכתי תתחזק היא <em>החיבור הישיר (Disintermediation) בין משקיעים וצרכני הון</em>. הגל הקודם של התפתחות במודלים של ML הביא לצמיחה משמעותית בהיקף האשראי הצרכני שהוסט לפלטפורמות הלוואות חברתיות (P2P Lending) שהצליחו לייעל ולדייק את תהליך החיתום ואיפשרו לקרנות ומשקיעים פרטיים להיכנס בנעלי הבנקים וגופי המימון המסורתיים. החיבור הזה היה מתבקש וטבעי בזכות שפע המידע המובנה (Structured data) על היסטוריית האשראי של לווים והיכולת של המודלים מ׳הדור הראשון׳ לייצר תחזיות על בסיסו.</p>
  <p>אם קראתם עד כאן אז קודם כל תודה! אני גם אשמח לקבל הערות ו/או הארות שיש לכם גם כדי להבין אם יש עניין בתוכן שלי בעברית.</p>
  <h3 id="רשימת-מקורות">רשימת מקורות:</h3>
  <hr>
  <ol>
  <li>Sandberg, D. (2025). Questioning the Answers: LLMs enter the Boardroom, <em>S&amp;P Global Market Intelligence</em>.</li>
  <li>Fama, E.F. (1993). Common risk factors in the returns on stocks and bonds. * Journal of Financial Economics*.</li>
  <li>Feng, G. (2020). Taming the Factor Zoo:: A Test of New Factors, <em>The Journal of Finance</em>.</li>
  </ol>
  
  

</body>
</html>
